{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translation pipeline with OpenAI Batch API "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "sys.path.append(os.path.join(os.getcwd(), \"..\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import asyncio\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Any, Optional, Union, List, Dict\n",
    "\n",
    "import tooldantic as td\n",
    "from tooldantic import ModelBuilder, OpenAiResponseFormatBaseModel\n",
    "from openai import AsyncOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translation pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove unnecessary data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths = [\n",
    "#     \"../data/english/ApolloCorpus\", \"../data/english/MashQA\", \n",
    "#     \"../data/english/MediQA_Task3\", \"../data/english/MedQuad\", \"../data/english/ViHealthQA\"\n",
    "# ]\n",
    "# files = [file for dir in paths for file in Path(dir).rglob(\"*\") if file.suffix in (\".json\", \".jsonl\")]\n",
    "\n",
    "# for path in files:\n",
    "#     # read files\n",
    "#     with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "#         data = json.load(f)\n",
    "\n",
    "#     if not data[0].get(\"id\"):\n",
    "#         continue\n",
    "    \n",
    "#     # delete id\n",
    "#     print(path)\n",
    "#     for d in data:\n",
    "#         del d[\"id\"]\n",
    "\n",
    "#     # write again\n",
    "#     with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "#         json.dump(data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prompt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\\\n",
    "You are the best medical translator in the world. Your task is to translate medical documents from English to Vietnamese. \\\n",
    "The translation must be clear, concise, and easy to understand, while preserving the original meaning and tone.\n",
    "\n",
    "Input will always be in JSON format, but the structure may vary. Your job is to translate only the values while retaining \\\n",
    "the keys and the structure of the input JSON exactly as they are. \n",
    "The output must strictly follow the same format as the input JSON. Provide only the translated JSON in the output, with no \\\n",
    "additional comments, explanations, or text.\n",
    "\n",
    "Examples 1:\n",
    "- Input:\n",
    "```json\n",
    "{\n",
    "    \"diagnosis\": \"Hypertension\",\n",
    "    \"prescription\": \"Take 1 tablet of Lisinopril 10mg daily.\",\n",
    "    \"notes\": \"Monitor blood pressure regularly.\"\n",
    "}\n",
    "- Output:\n",
    "```json\n",
    "{\n",
    "    \"diagnosis\": \"Tăng huyết áp\",\n",
    "    \"prescription\": \"Uống 1 viên Lisinopril 10mg mỗi ngày.\",\n",
    "    \"notes\": \"Theo dõi huyết áp thường xuyên.\"\n",
    "}\n",
    "```\n",
    "\n",
    "Example 2:\n",
    "- Input:\n",
    "```json\n",
    "[\n",
    "    {\n",
    "        \"doctor\": \"Dr. John Doe\",\n",
    "        \"appointment\": \"Annual physical exam\",\n",
    "        \"recommendations\": \"Patient should undergo a cholesterol test.\"\n",
    "    }\n",
    "]\n",
    "```\n",
    "- Output:\n",
    "```json\n",
    "[\n",
    "    {\n",
    "        \"doctor\": \"Bác sĩ John Doe\",\n",
    "        \"appointment\": \"Kiểm tra sức khỏe hàng năm\",\n",
    "        \"recommendations\": \"Bệnh nhân nên thực hiện xét nghiệm cholesterol.\"\n",
    "    }\n",
    "]\n",
    "```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create batching API**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "MAX_REQUEST_PER_FILE = 50000\n",
    "MAX_SIZE_PER_FILE = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batch_data(data: List[Dict], batch_size: int = BATCH_SIZE) -> List[List[Dict]]:\n",
    "    return [data[i : i + batch_size] for i in range(0, len(data), batch_size)]\n",
    "\n",
    "\n",
    "def create_batch_model(\n",
    "    data: Union[List[Dict], Dict],\n",
    "    model_name: str = \"Translation\",\n",
    "    model_desc: str = \"\"\"Translated output into Vietnamese\"\"\"\n",
    "):\n",
    "    example_data = data[0] if isinstance(data, list) else data\n",
    "    return ModelBuilder(base_model=OpenAiResponseFormatBaseModel).model_from_dict(\n",
    "        example_data, model_name=model_name, model_description=model_desc\n",
    "    )\n",
    "\n",
    "\n",
    "def get_object_size(batch: List[Dict]) -> int:\n",
    "    return len(json.dumps(batch).encode(\"utf-8\"))\n",
    "\n",
    "\n",
    "def prepare_request(\n",
    "    id: str,\n",
    "    batch_data: List[Dict],\n",
    "    model: str,\n",
    "    system_prompt: str,\n",
    ") -> List[str]:\n",
    "    batch_model = create_batch_model(batch_data)\n",
    "    class Translations(OpenAiResponseFormatBaseModel):\n",
    "        \"\"\"A list of translated output into Vietnamese\"\"\"\n",
    "        items: List[batch_model]\n",
    "\n",
    "    return {\n",
    "        \"custom_id\": id,\n",
    "        \"method\": \"POST\",\n",
    "        \"url\": \"/v1/chat/completions\",\n",
    "        \"body\": {\n",
    "            \"model\": model,\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": str(batch_data)}\n",
    "            ],\n",
    "            \"temperature\": 0.5,\n",
    "            \"response_format\": Translations.model_json_schema()\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "def prepare_batch_file(\n",
    "    dataset_path: Union[str, Path],\n",
    "    output_dir: str,\n",
    "    max_requests: int = MAX_REQUEST_PER_FILE,\n",
    "    max_file_size_mb: int = MAX_SIZE_PER_FILE,\n",
    "    batch_size: int = BATCH_SIZE,\n",
    "    model: str = \"gpt-4o-mini\",\n",
    "    system_prompt: str = SYSTEM_PROMPT\n",
    "):\n",
    "    # check file existence\n",
    "    if not os.path.isfile(dataset_path):\n",
    "        raise FileNotFoundError(f\"Input file not found: {dataset_path}\")\n",
    "    \n",
    "    # create output dir\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # load data\n",
    "    with open(dataset_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    data = data if isinstance(data, list) else [data]\n",
    "\n",
    "    # split data into batches\n",
    "    batches = create_batch_data(data, batch_size=batch_size)\n",
    "\n",
    "    max_file_size_bytes = max_file_size_mb * 1024 * 1024\n",
    "\n",
    "    # create batch files\n",
    "    path = Path(dataset_path) if isinstance(dataset_path, str) else dataset_path\n",
    "    res_batch_files = []\n",
    "    current_batch_file = []\n",
    "    current_size = 0\n",
    "    current_request_idx = 0\n",
    "    current_file_idx = 0\n",
    "    current_file_name = f\"{path.stem}-{current_file_idx}\"\n",
    "    for batch in batches:\n",
    "        request = prepare_request(\n",
    "            id=f\"{current_file_name}_request-{current_request_idx}\",\n",
    "            batch_data=batch,\n",
    "            model=model,\n",
    "            system_prompt=system_prompt\n",
    "        )\n",
    "        request_bytes = get_object_size(request)\n",
    "\n",
    "        # check size of current file\n",
    "        if current_size + request_bytes > max_file_size_bytes or len(current_batch_file) + 1 > max_requests:\n",
    "            # write it down\n",
    "            batch_file_path = os.path.join(output_dir, f\"{current_file_name}.jsonl\")\n",
    "            with open(batch_file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "                for line in current_batch_file:\n",
    "                    file.write(json.dumps(line, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "            res_batch_files.append(batch_file_path)\n",
    "            \n",
    "            # update counter\n",
    "            current_batch_file = []\n",
    "            current_size = 0\n",
    "            current_file_idx += 1\n",
    "            current_request_idx = 0\n",
    "            current_file_name = f\"{path.stem}-{current_file_idx}\"\n",
    "            \n",
    "            # update current request\n",
    "            request[\"custom_id\"] = f\"{current_file_name}_request-{current_request_idx}\"\n",
    "            request_bytes = get_object_size(request)\n",
    "\n",
    "        # add batch to batch file\n",
    "        current_batch_file.append(request)\n",
    "        current_size += request_bytes \n",
    "        current_request_idx += 1\n",
    "\n",
    "    # write remaining batch\n",
    "    if current_batch_file:\n",
    "        if current_file_idx == 0:\n",
    "            current_file_name = path.stem\n",
    "        batch_file_path = os.path.join(output_dir, f\"{current_file_name}.jsonl\")\n",
    "        with open(batch_file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            for line in current_batch_file:\n",
    "                file.write(json.dumps(line, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "        res_batch_files.append(batch_file_path)\n",
    "\n",
    "    return res_batch_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code is used for testing\n",
    "# batch_files = [\n",
    "#     {\n",
    "#         \"dataset_path\": Path(\"../data/translation_samples/input.json\"),\n",
    "#         \"batch_files\": prepare_batch_file(\n",
    "#             Path(\"../data/translation_samples/input.json\"),\n",
    "#             output_dir=\"../data/batch_files/translation_samples\",\n",
    "#             max_requests=1,\n",
    "#             batch_size=1\n",
    "#         )\n",
    "#     }\n",
    "# ]\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "data_dir = \"../data/english\"\n",
    "output_dir = \"../batch_files\"\n",
    "dataset_dirs = [Path(os.path.join(data_dir, sub_dir)) for sub_dir in os.listdir(data_dir)]\n",
    "\n",
    "batch_files = []\n",
    "for dir in dataset_dirs:\n",
    "    datasets = [file for file in Path(dir).rglob(\"*\") if file.suffix in [\".json\", \".jsonl\"]]\n",
    "    \n",
    "    for ds_path in datasets:\n",
    "        batch_files.append(\n",
    "            {\n",
    "                \"dataset_path\": ds_path,\n",
    "                \"batch_files\": prepare_batch_file(\n",
    "                    ds_path,\n",
    "                    output_dir=os.path.join(\"../data/batch_files\", ds_path.parent.name)\n",
    "                )\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def process_batch_file(file: str, client: AsyncOpenAI = AsyncOpenAI()):\n",
    "    # upload files\n",
    "    input_file = await client.files.create(\n",
    "        file=open(file, \"rb\"),\n",
    "        purpose=\"batch\"\n",
    "    )\n",
    "\n",
    "    # create batch\n",
    "    response = await client.batches.create(\n",
    "        input_file_id=input_file.id,\n",
    "        endpoint=\"/v1/chat/completions\",\n",
    "        completion_window=\"24h\"\n",
    "    )\n",
    "\n",
    "    return response.id\n",
    "\n",
    "\n",
    "async def process_dataset(item, client: AsyncOpenAI = AsyncOpenAI()):\n",
    "    dirname: Path = item[\"dataset_path\"]\n",
    "    batch_ids = await asyncio.gather(*[process_batch_file(file) for file in item[\"batch_files\"]])\n",
    "    return {\n",
    "        \"dataset\": f\"{dirname.parent.name}/{dirname.name}\",\n",
    "        \"batch_ids\": batch_ids\n",
    "    }\n",
    "\n",
    "\n",
    "client = AsyncOpenAI()\n",
    "dir_to_batch_ids = await asyncio.gather(*[process_dataset(item) for item in batch_files])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/batch_ids.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(dir_to_batch_ids, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise SystemExit(\"Stop right there!\\nYou have to run the rest manually\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle over enqueued token limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/batch_ids.json\", \"r\", encoding=\"utf\") as file:\n",
    "    batch_ids = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_batch_ids = [id for item in batch_ids for id in item[\"batch_ids\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches/batch_67752ab889c88190a02f8f372cce7994 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches/batch_677529c935cc8190968763d5e4d3af17 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches/batch_677529bb9958819081b9556ee39c1c90 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches/batch_67752a833f088190839baf9ef1e31e71 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches/batch_677529e736548190928b739e20129b8d \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches/batch_677529be7ba08190bb49d33884570070 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches/batch_677529b00be88190be98e384c9012a49 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches/batch_677529b9b91881909370367a2e9a675f \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches/batch_677529b6dd38819096267e7af65e3b1c \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches/batch_67752a8577788190947491d05dede971 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches/batch_677529b3db008190ab4e1fbed86ba9a0 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches/batch_67752ab4ba2c8190b0d39e1232c2d75f \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches/batch_67752a8b905c81908aa98b753a5b3a90 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches/batch_677529b1c9b481909bd6ac4dd20f434d \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches/batch_67752ab4c35081908171f85d424dab59 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches/batch_67752a921ae48190aa52385515b53628 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches/batch_677529af142881908af3b67b24b1d6cd \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches/batch_67752aaa3604819080d2fcb771333e43 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches/batch_67752a71af78819086099424089cc336 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches/batch_677529db4b3c8190a02bd6e4cfea002f \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches/batch_67752ab8b0508190bb075079bc6e0416 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches/batch_67752a3b075481909c53992f75256c99 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches/batch_677529c886008190a392b0c668b6e470 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches/batch_67752aac6b68819080a108be5b97259f \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches/batch_677529b12c688190a9b623aef3a0305a \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches/batch_67752a6566548190b35b404460ae81e8 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches/batch_67752ab31b9c8190917ceb0dbcf8e0bf \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches/batch_677529b3b18881909326f476f4e6213c \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches/batch_677529b4af948190a3c554536f07f08a \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches/batch_67752a5d5d808190b8fb07a7f0737177 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches/batch_677529b1cb1081908fd14b96598c1b5f \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches/batch_67752a94ac60819089eb19e6c8d165ab \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches/batch_67752a680f0481909f4fd210346573d6 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches/batch_677529b397a08190a2311c5db270eb1d \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches/batch_677529c81ebc81909d364681eaa84711 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches/batch_677529be75e48190b28ea57af524cb05 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches/batch_67752aae85bc8190975cc3fe142ba9fa \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches/batch_67752a2451e08190a870cdb04f05dc8a \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches/batch_67752ab06c348190954dc0fac233eef5 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches/batch_677529d262288190a69bc1f7e7221ff0 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches/batch_67752aae61848190865c081427e2586c \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches/batch_677529d5314c8190ab62213c5eb6c6b3 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches/batch_677529b4e9e081909f741f25997f32f1 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches/batch_67752ab8402481909529fb1c336c2c95 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches/batch_677529aeeb088190bd881f7a620101f1 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches/batch_67752abbc5048190a5fa18b77a2c320a \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches/batch_67752aa10b188190bb139d5a1e9cb0d9 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches/batch_67752aa9db6081908ee3197349d39ff7 \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "client = AsyncOpenAI()\n",
    "batch_reponses = await asyncio.gather(*[\n",
    "    client.batches.retrieve(id) for id in total_batch_ids\n",
    "])\n",
    "batch_input_file_ids = [response.input_file_id for response in batch_reponses if response.errors and response.errors.data[0].code == \"token_limit_exceeded\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/batches \"HTTP/1.1 400 Bad Request\"\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': 'Billing hard limit has been reached', 'type': 'invalid_request_error', 'param': None, 'code': 'billing_hard_limit_reached'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m batch_responses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;241m*\u001b[39m[\n\u001b[1;32m      3\u001b[0m         client\u001b[38;5;241m.\u001b[39mbatches\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m      4\u001b[0m             input_file_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mid\u001b[39m,\n\u001b[1;32m      5\u001b[0m             endpoint\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/v1/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m             completion_window\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m24h\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m         )\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mid\u001b[39m \u001b[38;5;129;01min\u001b[39;00m batch_input_file_ids]\n\u001b[1;32m      9\u001b[0m )\n",
      "File \u001b[0;32m~/Code/Vietnamese-Healthcare-Assistant/.venv/lib/python3.12/site-packages/openai/resources/batches.py:301\u001b[0m, in \u001b[0;36mAsyncBatches.create\u001b[0;34m(self, completion_window, endpoint, input_file_id, metadata, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    267\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    268\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Batch:\n\u001b[1;32m    269\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;124;03m    Creates and executes a batch from an uploaded file of requests\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;124;03m      timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 301\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[1;32m    302\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/batches\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    303\u001b[0m         body\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[1;32m    304\u001b[0m             {\n\u001b[1;32m    305\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompletion_window\u001b[39m\u001b[38;5;124m\"\u001b[39m: completion_window,\n\u001b[1;32m    306\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mendpoint\u001b[39m\u001b[38;5;124m\"\u001b[39m: endpoint,\n\u001b[1;32m    307\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_file_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: input_file_id,\n\u001b[1;32m    308\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[1;32m    309\u001b[0m             },\n\u001b[1;32m    310\u001b[0m             batch_create_params\u001b[38;5;241m.\u001b[39mBatchCreateParams,\n\u001b[1;32m    311\u001b[0m         ),\n\u001b[1;32m    312\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[1;32m    313\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m    314\u001b[0m         ),\n\u001b[1;32m    315\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mBatch,\n\u001b[1;32m    316\u001b[0m     )\n",
      "File \u001b[0;32m~/Code/Vietnamese-Healthcare-Assistant/.venv/lib/python3.12/site-packages/openai/_base_client.py:1843\u001b[0m, in \u001b[0;36mAsyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1829\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1830\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1831\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1838\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_AsyncStreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1839\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _AsyncStreamT:\n\u001b[1;32m   1840\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1841\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1842\u001b[0m     )\n\u001b[0;32m-> 1843\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls)\n",
      "File \u001b[0;32m~/Code/Vietnamese-Healthcare-Assistant/.venv/lib/python3.12/site-packages/openai/_base_client.py:1537\u001b[0m, in \u001b[0;36mAsyncAPIClient.request\u001b[0;34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[0m\n\u001b[1;32m   1534\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1535\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1537\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m   1538\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1539\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   1540\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1541\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1542\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1543\u001b[0m )\n",
      "File \u001b[0;32m~/Code/Vietnamese-Healthcare-Assistant/.venv/lib/python3.12/site-packages/openai/_base_client.py:1638\u001b[0m, in \u001b[0;36mAsyncAPIClient._request\u001b[0;34m(self, cast_to, options, stream, stream_cls, retries_taken)\u001b[0m\n\u001b[1;32m   1635\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39maread()\n\u001b[1;32m   1637\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1638\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1640\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1641\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1642\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1646\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1647\u001b[0m )\n",
      "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': 'Billing hard limit has been reached', 'type': 'invalid_request_error', 'param': None, 'code': 'billing_hard_limit_reached'}}"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/batches \"HTTP/1.1 400 Bad Request\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/batches \"HTTP/1.1 400 Bad Request\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/batches \"HTTP/1.1 400 Bad Request\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/batches \"HTTP/1.1 400 Bad Request\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/batches \"HTTP/1.1 400 Bad Request\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/batches \"HTTP/1.1 400 Bad Request\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/batches \"HTTP/1.1 400 Bad Request\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/batches \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/batches \"HTTP/1.1 400 Bad Request\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/batches \"HTTP/1.1 400 Bad Request\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/batches \"HTTP/1.1 400 Bad Request\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/batches \"HTTP/1.1 400 Bad Request\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/batches \"HTTP/1.1 400 Bad Request\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/batches \"HTTP/1.1 400 Bad Request\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/batches \"HTTP/1.1 400 Bad Request\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/batches \"HTTP/1.1 400 Bad Request\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/batches \"HTTP/1.1 400 Bad Request\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/batches \"HTTP/1.1 400 Bad Request\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/batches \"HTTP/1.1 400 Bad Request\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/batches \"HTTP/1.1 400 Bad Request\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/batches \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/batches \"HTTP/1.1 400 Bad Request\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/batches \"HTTP/1.1 400 Bad Request\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/batches \"HTTP/1.1 400 Bad Request\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/batches \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
